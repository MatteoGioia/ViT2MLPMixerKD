# ViT2MLPMixerKD

## References

### Distillation

- Distilling the Knowledge in a Neural Network -  https://arxiv.org/pdf/1503.02531.pdf

### Vision Trasformers

- An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale - https://arxiv.org/pdf/2010.11929v2.pdf 
- Training data-efficient image transformers & distillation through attention - https://arxiv.org/pdf/2012.12877v2.pdf

### MLP Mixer

- MLP-Mixer: An all-MLP Architecture for Vision - https://arxiv.org/pdf/2105.01601v4.pdf

### Code attributions

- Huggingface (ViT): https://github.com/huggingface/transformers
- Google (MLP Mixer in jax): https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_mixer.py
- rishikksh20 (MLP Mixer in torch): https://github.com/rishikksh20/MLP-Mixer-pytorch

### Dataset

- ImageNet1K: https://www.image-net.org
- Cifar100: https://www.cs.toronto.edu/~kriz/cifar.html
