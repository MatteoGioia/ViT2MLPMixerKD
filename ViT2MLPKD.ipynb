{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL7l7ZJI61rQ"
      },
      "source": [
        "# Knowledge distillation\n",
        "In this notebook, we will try to distill the knowledge of a Vision Transformer into a MLP Mixer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES_z2lrFzS4Q"
      },
      "source": [
        "##0. Downloads and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ol5yDer7LaO",
        "outputId": "1c4ae6e8-e502-4486-bc13-a6858eeb5a5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.8/dist-packages (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Downloads\n",
        "!pip install -q transformers datasets[vision] pytorch-lightning\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WumRkwLd7SKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e6720a-7fbe-48c8-c00c-fa9b8f0834ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "\n",
        "# general imports\n",
        "from typing import *\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pytorch imports\n",
        "import torchvision\n",
        "import transformers\n",
        "#from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdzEa5BqwFOy",
        "outputId": "a7dbfafb-764a-4499-9fcd-20cc4253e594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ViT2MLP\n"
          ]
        }
      ],
      "source": [
        "#@title Move to the project folder\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "if not os.path.exists(\"ViT2MLP\"):\n",
        "  os.mkdir(\"ViT2MLP\")\n",
        "\n",
        "%cd ViT2MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yAlyRDW8fwM",
        "outputId": "81db3acf-c5ef-4937-e2ce-2f351c95d682",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#@title Download the datasets\n",
        "# TODO need to download it\n",
        "#imagenet = torchvision.datasets.ImageNet(root = \".\",split = \"val\")\n",
        "\n",
        "cifar100_test = torchvision.datasets.CIFAR100(root = \".\", train = False, download = True)\n",
        "cifar100_train = torchvision.datasets.CIFAR100(root = \".\", train = True, download = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MLtary4p_Z_B"
      },
      "outputs": [],
      "source": [
        "#@title Visualization function\n",
        "def visualize(datapoint : int, dataset : torchvision.datasets):\n",
        "    image, label = dataset[datapoint]\n",
        "    plt.title(f\"Ground truth: {dataset.classes[label]}\")\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjYPKIo8zW_7"
      },
      "source": [
        "##1. Code for the pre-trained transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcazhnqFv0Lx"
      },
      "source": [
        "References:\n",
        "- [main transformer docs](https:huggingface.co/docs/transformers/index) \n",
        "- [ViT Docs](https://huggingface.co/docs/transformers/model_doc/vit)\n",
        "- [lighting finetuning example](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKpdGxL59-Is"
      },
      "source": [
        "### Example of transformer model\n",
        "\n",
        "Load ViT finetuned on Imagenet1k or CIFAR100. To load the non fine-tuned version use **\"google/vit-base-patch16-224-im21k\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WsSKp30aAMah"
      },
      "outputs": [],
      "source": [
        "#@title Load the model\n",
        "cifar100_vit = \"Ahmed9275/Vit-Cifar100\"\n",
        "imagenet1k_vit = \"google/vit-base-patch16-224\"\n",
        "tuned_on : str = \"cifar100\" #@param['cifar100', 'imagenet1k']\n",
        "if tuned_on == \"cifar100\":\n",
        "    model_str = cifar100_vit\n",
        "else:\n",
        "    model_str = imagenet1k_vit\n",
        "\n",
        "\n",
        "feature_extractor = transformers.ViTFeatureExtractor.from_pretrained(model_str)\n",
        "\n",
        "transformer_model = transformers.ViTForImageClassification.from_pretrained(model_str).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW2bXu6dAYls"
      },
      "outputs": [],
      "source": [
        "#@title Function to predict - a questa funzione dovremmo anche passare il modello :)\n",
        "def predict(image, only_label : bool = False) -> str:\n",
        "    inputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = transformer_model(**inputs).logits\n",
        "\n",
        "    # model predicts one of the 1000 ImageNet classes\n",
        "    predicted_label : int = logits.argmax(-1).item()\n",
        "    if only_label:\n",
        "        return predicted_label\n",
        "    return transformer_model.config.id2label[predicted_label]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "RBCle0TPyy2b",
        "outputId": "92468a84-8b9b-4c2d-a185-405f97f8fd76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: lion\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5Bc9XXnv6df09PdM9Pz0mj0QkjIgHBAgNBiAw52wMEQL/auywtOsWyKWN6Nvbuu2JWl7FRCst4Nycav2nK8kWNikmAwjnEBDkmMWQgG28DwkHjq/RiN5qV5d/f09OOe/aOv1iPV73tnpJnpEdzzqeqant/p373n/vqee7t/3z7nJ6oKwzDe+USW2wHDMOqDBbthhAQLdsMICRbshhESLNgNIyRYsBtGSLBgDxEisl5EVERidd7vUyLy22fY91oROTrr/9dF5NpFcy5EWLAvMiJyi4g8JyJ5ERnyn/+OiMhy+zYXInJIRK5b4DbuEpG/WyyfTkVVL1LVp5Zq++9kLNgXERH5HICvA/hfAFYC6ALwHwFcBSBB+kTr5uACqfcnAmORUVV7LMIDQAuAPIB/O8frvgPgmwAe819/HYALATwFYBzA6wD+9azXPwXgt2f9/x8APDPrf0XtgrLX7/8NAOLbogD+HMBxAAcAfNp/fczh198C8ABMA8gB+D0A6/3X3wHgCICnAVwL4OgpfQ/5x3EDgBKAsr+NnbOO4b8DeBbAFIAfA+iY57ietL8T+/KfNwD4GoBj/uNrABpm9wPwOQBDAPoB/NZynyfL+bA7++LxHtROvofn8dpPAPgfAJoAPAfgUdQCYAWA/wzgPhE5/zT2/RsArgBwMYCPA/h1v/2Tvu1SAFsBfIxtQFVvQy2gP6yqGVX9s1nmX0XtgvTrzs6/3MY/AfifAL7nb+OSWeZPAPgt1I4xAeDzJwwisktEPjGP4zyVLwK4EsAWAJcA2Abg92fZV6J2EV6N2gXrGyLSegb7eUdgwb54dAA4rqqVEw0i8jMRGReRaRF536zXPqyqz6qqh9qJmgFwt6qWVPX/AvgRgFtPY993q+q4qh4B8KS/TaAW+F9T1V5VHQXwJ2d4bHepal5Vp8+wPwD8taru8bfx4CwfoaoXq+p3z2Cbvwngj1V1SFWHAfwRgNtm2cu+vayqj6H2aeN0LqLvKCzYF48RAB2zv9eq6ntVNevbZo9176znqwD0+oF/gsOo3Y3my8Cs5wXULh7/f9unbPdM6J37JXPCfFwIq3DyMR32204wMvviu4j7fVtiwb54/BzADICb5/Ha2amGxwCsFZHZ78U6AH3+8zyA1CzbytPwqR/A2lO2O1+/WPtJ/vgTjJ3z2MZScAzAObP+X+e3GQ4s2BcJVR1H7WPkX4jIx0SkSUQiIrIFQDqg63Oo3XF+T0Tivob8YQAP+PZXAPwbEUmJyHmoffecLw8C+C8issb/rnrnHK8fBLBhjtfsAZAUkZtEJI7ad+SGU7ax/pSL11JxP4DfF5FOEekA8AcAlkz2e7tjwb6I+JNav4vaTPag//hLAP8NwM9InxJqwf0h1GbN/wLAv1fVt/yXfBW1Ge5BAPcCuO80XPoWgH8GsBPASwAemuP1f4Ja8IyLyOddL1DVCQC/A+CvUPv0kUdt1vsE3/f/jojIS/Nx0v+hzG/O57Wn8CUAPQB2AXgVtWP80hlsJxSckGgMw3iHY3d2wwgJFuyGERIs2A0jJFiwG0ZIqGtiQzKZ0HS60W0MuOxUvarbcNLvUE7ZXJRvMBZx5qQAAJpSHdyPint/0RgfxiA/olGeAxML2GaQquV5ZWd7cWKI9ol4JW6LcB+np/kP6qqVirO9XCHvJYCgyeJolCcNxgPGUZgtEqd9Ksq3F4k1UFuZHHPNkaCkR/dxe+R8q/nh9nEyN4np4rRzZwsKdhG5AbUsryiAv1LVu4Nen0434kM3XeneVpIPxlR+ytleKRdon1Qzl7bbm/mP0z5w6SepbXx40tme7VhB+zS2cD+yLU3U1tbJt5mI84vVdH7Y2b770f9N+zSU+A/kUinu4+s736C23Oigs71vZIL2KZf4yZ1t4sHZ1cp/FNeQbnO2a5qP77DHfz6faj+X2o4NH6e2aCLgglR1XySmJvK0T1NLu7P9vn/gyuwZf4z3fzn1DdT04c0AbhWRzWe6PcMwlpaFfGffBmCfqh7wfxjyAOb3U1HDMJaBhQT7apycIHEUjuQNEdkuIj0i0lOc4d8NDcNYWpZ8Nl5Vd6jqVlXdmmzg3zUNw1haFhLsfTg5o2oNfpmpZRjGWcZCZuNfALBJRM5FLchvQa0aCSUSF2Q63dJbseSecQeAZMU9U+9FiIwHIJvmM7TrW6+htpH+EWqLN7n9iCT5LHIiQKpBlM8wj425Z/4BYHRogNoqx152b+/obtpn7aoWaus9wvc1NDBKbYeH3bPusTi/v1y+qZvaygHS1av7uY+XX+ieWY8qV3IqM1xBGR8fp7bJMX7uTFdmqK0l0+ze3hSfjS+U3DP4lTKX/8442FW1IiKfQS2rKgrgHlV9/Uy3ZxjG0rIgnd0v9fPYIvliGMYSYj+XNYyQYMFuGCHBgt0wQoIFu2GEhPou8KcRRKruH9aszq51tgPAcMKdYJBo4BJJ08w51LbrF85ycACANWt5osOVV73f2d7ZwYu2siw0ANAi/0VhbopLb7mjB7jt8PNuQ5VLP5MTPHutmOeJK9EElxwzTW7pc9u7eT3LZJUfc6qVZyMem+KZdL2D7m1e+C633AUAkTI/rqkCl+wSCS6lBiQPAlX3/pqyPAmprG6JLajMp93ZDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXWfjAUUk4p55bEi4ywcBQGzSPVt86BBP7pg69Ca1jY/wZIajR/g2X+75qbP9vPO3ONsB4MprPkRte97YSW0DATPul23spLZVKffM9Jv7eZJGazZJbTHhisHIFJ/Fv/B896z7+Zu5SvLE069QW3eCJ8Jc//73Utvf3e9eQbstyxOUWtp5ySqNcFUg1ZyitskxXgOwf8BdFmwmz4+5OOMe+0pAHTy7sxtGSLBgN4yQYMFuGCHBgt0wQoIFu2GEBAt2wwgJdZXePE8xVXAnhkyXj9J+hSl3MkbfMZ6k4eX4UkLlKk9O8apcapqadCdBvPryU7QP8x0Atm67itqquX5qaxAuHa5a4U4OOpQIWE5K+Xgc6eN15mIBWRedWfeKJYjx5KVLL+ZJMkf6+GorqRSvRXj+ho1ug/DzI97IpchknO9r9DivhTccYCuTcy6X5+dicTrnbPeqPCnI7uyGERIs2A0jJFiwG0ZIsGA3jJBgwW4YIcGC3TBCQl2lt3IFOD7oljwaM1wKiUXcGUrZVr5c0OgUz/JScEkjnuCZSw0N7iykIOnqeP8+atu/2700EQA0R4vUtq6L+5hOu7MKN67n2VrJOC+QVpzmteuyLbxG2hhZuujQT1+ifVZ1clkuE+f13R55ktTdA9Aac2eBNQVk0UXifHz7A2rQlUs8CzDbmqW26Rn3+M+UeE0+EbePkQi/fy8o2EXkEIApAFUAFVXdupDtGYaxdCzGnf39qsp/8WAYxlmBfWc3jJCw0GBXAD8WkRdFZLvrBSKyXUR6RKSnNMO/KxuGsbQs9GP81araJyIrADwuIm+p6tOzX6CqOwDsAICW1iyfhTMMY0lZ0J1dVfv8v0MAfghg22I4ZRjG4nPGd3YRSQOIqOqU//yDAP44qI9WFaW8WwpRspwNACDq/kDQ1sELA8bJMlMA0Jd3y0IAUK5yySuVchfFrMzwTCOW0QQA0wEZcas7+VuTbODLE3lkmae153DpLX+cy5TvWssloz3HpqjtQL87W66fyEwAkMlyee2C1Tzb7GiZZ6lVRtzHlmnhHzIbm7nc2DfDl41Kda6htqEhntU5MTHmbFeuDmJszC3LVQKy3hbyMb4LwA9F5MR2vquq/7SA7RmGsYSccbCr6gEAlyyiL4ZhLCEmvRlGSLBgN4yQYMFuGCHBgt0wQkJds94UHmaq7qyh0UEu/5xz7jpne1S4VNOQ4odWES6vVTxu89QtyUiUXzPZmlwAUJjkhSPLTdz/qHCpqVxxSy9dq0gBSABe0V28EAA2ruf9Csqzww4MuGW5ay67gvbpSHP5tX0Vz4j7lRiX0V4eHnS2Z1tbaJ9khsuDGOQZjtGAYpS5PJcpp4vu86opHZAV2er2I3qEn4t2ZzeMkGDBbhghwYLdMEKCBbthhAQLdsMICfWdja96KE24ZyUvuvRC2i9fcM9ax6M8ySTWxBNhUik+i5xQPqM6POwuyJNO8tnxTENA7bECn6mPkBpjAFCY5Ak02dXu/VXBZ6xbuniSjCrv9+5UF7UNF/c720eGh2ifbIKPVanM30/1+Ex3qtH9fsYT/D1ra+LHXCy6k1YAoORxNaEC/l7HEu5j62jniV4VcSdDRaNcSbA7u2GEBAt2wwgJFuyGERIs2A0jJFiwG0ZIsGA3jJBQV+mtUq1gZNwtXQwP8qSQZNwtTTQFLBdUnua1uNrb3bXkACDlceliw5pOZ/uqFStpn4NHuIwzMNhHbZOT3P9K2b0cFgDEYm7/CwHLOMWEX/MlxiWveJTXwltBpM9ShMulxQrf3vFh7n8iYPmq1ozbj5mAfTUE+Igyl976R/hyTUH31WLRnXwVVL8wRpLA/DJxp+mBYRjvKCzYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXaW3WCyC9g53FlJ3mktN565ucrZnA6SfhirPMrr+8rXUtmoFz7xq73Bnh43nuB/39O2ltsmA5Z8mpoKuw24JEADGyFJOEuHyVCJgHKMNPPsuGeEyz5oV7vppowFjtWfPPmp7dIDLUJ0BNeO2bHC/n7GAuoGRGJc2V2a5/72jvJaf5/F6iaMTbtk528LPD89zS7pBWYpz3tlF5B4RGRKR12a1tYnI4yKy1//LK+MZhnFWMJ+P8d8BcMMpbXcCeEJVNwF4wv/fMIyzmDmD3V9v/dQlOW8GcK///F4AH1lkvwzDWGTO9Dt7l6r2+88HUFvR1YmIbAewHQASCf69xTCMpWXBs/FamxGgswKqukNVt6rq1njAb5gNw1hazjTYB0WkGwD8v7ywmGEYZwVn+jH+EQC3A7jb//vwfDp1pRvw+fee57StaHPLawDQmHS7KRLgfoRLRhqQGZTL8eV9BodYVhO/ZjY1cKlmZoYvNTUxxYsojo3x7Ko43IUUkwFFNqMp7n/zSi7zdTVzmVIT7vdzZu9B2qdxgI/VdICU2tbKCzN2dbiXeSoHLMvFRWCgo5mP43SRZzFWK3yMp4tuXyan+Psci7rPfU/5GM5HersfwM8BnC8iR0XkDtSC/HoR2QvgOv9/wzDOYua8s6vqrcT0a4vsi2EYS4j9XNYwQoIFu2GEBAt2wwgJFuyGERLqmvXWkGzEeZvca7olWwKkobj7msTze4BYnP9aTz2endRa5oUN9+w67GznQh6QjvL1v4IKNg4GZFANDHJJJp1yH3ckQF6LebyoZJCUM36cF1+czrmlw4O9vE9b92pqe1ec+9EeUHg0HnVLqU2tvOhole8KmQw/r4aHhqmtWOHnQYScB4VSnvaJN7j9WJD0ZhjGOwMLdsMICRbshhESLNgNIyRYsBtGSLBgN4yQUFfpTRFBkWSjTY1yyaucc0sQE2N8fTgFlyCas7yg4MZz3QUxAaCry53J1dfLZbKYcvmkrZH7eHSYy2sHB3iGYHd3s7O9oRQgASZ4nYFyjhd6lEb3vgAg3UzGapCPVT7uzlADgGijO5sPANq6eAHRePSosz3RmKF9KlUupiYCbo/pgMzNwjg/V9eu2eD2I0AGLheIbSEFJw3DeGdgwW4YIcGC3TBCggW7YYQEC3bDCAl1nY2vlEo43uuu0xWQe4CpfMHZPl3gdcRWdPKZ0WScz+yOj/C6cBHP7Uc1qD5aC0+c6Grhw390NGCm/jif2c3l3DXjvKDCaspnwb0K9yPG80/gzbh3uO1SPnNeTq6hNknwGfJVrXyMIwX3GI9N8Bp/aeWKTDLBz52VHfycGxg/demFX8Im0Fs7eG293sNulSEoOczu7IYREizYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCfaW3ahWjY24JohJQo6u5xZ1w0dLNlyZKxLgI4QUJfcplnKjn3mY+z5M7PI/vK5Piw98UIDUVeG4K9h457myPKL+uXwB+zM3NfBxj+QlqSyTcyUar16ykff7l2R5qG+wfobYrLnEnkgDAOrK/aoUv81WY4vKrF+H1+lZkuCzX3szlzdyke6nEzq522icecUubQfUQ57P80z0iMiQir81qu0tE+kTkFf9x41zbMQxjeZnPx/jvALjB0f5VVd3iPx5bXLcMw1hs5gx2VX0aAP/5j2EYbwsWMkH3GRHZ5X/Mb2UvEpHtItIjIj1T0zwZ3zCMpeVMg/2bADYC2AKgH8CX2QtVdYeqblXVrU2NvEKMYRhLyxkFu6oOqmpVVT0A3wKwbXHdMgxjsTkj6U1EulW13//3owBeC3r9CTzPQ45kqpVK/CO+xEiNNNYOIBLlMkiVq3zI5bkfkaJbkskFZFCVily6SkT58KcbuG1qkmfZ7dzt9jHZEPCpKiCTa+O6LmrL5/hyR+/afL6z3RN+f1m3li//NJPnxywBWYzTJbf06REZFQCKuSDpjZrQ0sjTADPNvObdxMigs328z53ZBgCi/NxnzBnsInI/gGsBdIjIUQB/COBaEdmCWkbdIQCfOu09G4ZRV+YMdlW91dH87SXwxTCMJcR+LmsYIcGC3TBCggW7YYQEC3bDCAl1z3obn3JnSnlVLoWMTLgLPXoyQPuce846amtr5lLNTI7/Mjhecvs+Oc6z3ope0BAHyHJxLq0MjfIlpYbH3VJTVxvPh3prH5d4yiWeHVaZ4RJVpoVleQVkI3rcx42beGbbRIAsl0i6dVat8NTBfI7Lr0GKlxcgszZFeWZhUdznSKzEq4Sm4u6imJEAadPu7IYREizYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXaW3crmCvn63tBWJ8OtOosEtlVUCpJq+wTFqy+e4DFLJ837ZhFuuKZe59FOqBlxPlfvfGFBwUtniYAAmy265JjXD5SRvjKcBqvLMtsYGrkO19JECkcrlpPFxXsDyiksvpLbRMf6eFYl02JZ1FzEFgIHhXmqbDMg4zDRnqW1zZwe1Hai43+tVnbzg5HTZ/X6+ECDZ2p3dMEKCBbthhAQLdsMICRbshhESLNgNIyTUdTZeVTFDZotbmnhySnPaPXteBV+KZ2qKJ6fkJvnMdDbJZzPzVfdsfICQgOoMTyQpF/nMdCbBjy2d4G/bRNE9s54j7QDgxfnsfkOR+y8RrmocJapLIqC2XqHAl8rad5irAi1ZvrTSW3uPONsvueg82scLCIvR3CS1dXfzen3XXPevqC15zkXO9pefeYX2Gend72xviPP3xO7shhESLNgNIyRYsBtGSLBgN4yQYMFuGCHBgt0wQsJ8VoRZC+BvAHShVkBsh6p+XUTaAHwPwHrUVoX5uKryjAQAVVXkqJTD66qVSu5aZ6k0T2aYnOTSGxeagDSpBwYA+Yhbeks1ctnQ8wLqmQX4UVUuQ3U286WcJklptekql/lUAxJhJt31/wAgKtyPouce/0w6RftEhMueBwe45LU6wsd/aNx93LuP8FM1EZCgNFHg7+dEnsuU2W6eCLPmwpXO9o3vvoX2efYfe5ztjY8+QfvM585eAfA5Vd0M4EoAnxaRzQDuBPCEqm4C8IT/v2EYZylzBruq9qvqS/7zKQBvAlgN4GYA9/ovuxfAR5bKScMwFs5pfWcXkfUALgXwHICuWSu5DqD2Md8wjLOUeQe7iGQA/ADAZ1X1pC9QWqum4PwKKiLbRaRHRHpmKvx7qGEYS8u8gl1E4qgF+n2q+pDfPCgi3b69G8CQq6+q7lDVraq6tSFmk/+GsVzMGX0iIqgt0fymqn5llukRALf7z28H8PDiu2cYxmIxn6y3qwDcBuBVETmRhvMFAHcDeFBE7gBwGMDH59pQyVP0Ft3aUI5kSQFAssEt8cSq/c52AGhu5FljjUkuGa1duYba2pszzvbCyBTto8olxdYkz1BqDHhnNCDbrEC+Ko3kuKw1Mc2XQioFLJOUjPN6bLGye3+5AOkqSJYrV7ltaJiPf2drm7M9WubyWraLTz9NvNVHbYd7j1Hb1ASX5YZed8toU1hB+2RWuMcjEuf37zmDXVWfAcBG5tfm6m8YxtmBfYk2jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1LTjpCTDd4M5Cqma4VOaRrLJ9h8Zpn9VVLq1sbuHL9MQamqittcNt62hJ0z7V6DpqKxR53tuxw3upbXWAvLKiq9vZ/sQLe2if4YBCj+pxH0fyXHprSbvHpFTkWXRV5cc1wxVAVAN+mdm1sdPZ3tnCT/13X76B2kYDxmPo8AC1/ewXu6ntvE0bne3F3FO0z8ED7kzQ/DiPCbuzG0ZIsGA3jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1ld7iiRhWrnMX1ysErM0WK7mvSdGA7K/JADmmuZmvDda+YjW1rVztlgc7mngxwcYO9/ECQKqzndomCtdRG8r82CKt5zjbd33qi7RPX9Et4wDAZI7bZgo8k4t5mIxxifX4OM9eSycD9lXhmXSDQ+5zREs8i25tP19X7sM3XkNt33voZWqLBhQyfev1fc72LRcFZDeW3GPlBUiDdmc3jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1nY2HCDTqnmE8PsJn4yvj7iyI7gxf9meqzJc0Wr9pPbU1JhupLT/tnulsbeF99r+xk9o0FrAUUtad0AIA6fQqaps41uvuk2qlfVqzPHkiHuXH1tXOVYjuVnciTCbJ3zMVfu9pTPBTNRHnSU+9R9zjoRE+a31w32Fqa1/F68Ldcuv11PbCUz+ltpmie2mrUoEnUUUayLkT4WNod3bDCAkW7IYREizYDSMkWLAbRkiwYDeMkGDBbhghYU7pTUTWAvgb1JZkVgA7VPXrInIXgE8COJE18AVVfSxoW17VQ37CXYMsGQtYCqnDLf+kAxJhMlWeOJFpbaa2fG6E2vYddydc7D/CkztKyiWvxo711FYZ5ckuTRl3HT8AKFXcdeG2XHE17bO5cBm1tTXxpbI2XXQetSVibv9TGfcSWgDQ0MilSKGLEgGVMk+E+evvfNfZ/vzzz9M+5YCCd41pLh1e3cZrEa7q4udcpeju1z/K6/VVYm5ZToWfi/PR2SsAPqeqL4lIE4AXReRx3/ZVVf3zeWzDMIxlZj5rvfUD6PefT4nImwB4HqhhGGclp/WdXUTWA7gUwHN+02dEZJeI3CMi/POqYRjLzryDXUQyAH4A4LOqOgngmwA2AtiC2p3/y6TfdhHpEZGecpl/1zQMY2mZV7CLSBy1QL9PVR8CAFUdVNWqqnoAvgVgm6uvqu5Q1a2qujUe52uEG4axtMwZ7CIiAL4N4E1V/cqs9tmZGh8F8Nriu2cYxmIxn9n4qwDcBuBVEXnFb/sCgFtFZAtqctwhAJ+aa0PJaAIXZN1ze41rLuROxtzyTy7PM9skxbPo+ia5VNPW9h5qa9ywxtneRJanAoBkQGbeyi6+1FQ0xj8FFae5rBhRt+TVPTZI+wyO9lNbCrwGnSjPHCsW3P3K4OOBqSFqyjTzKaGHH/1nahs47q7VlkhyCXAkoO7eyDhf8qr3rUPUVhH+fj76lLt23Z5D/H15302fdbZXPS5Rzmc2/hnAKXIGauqGYZxd2C/oDCMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXQtOZprb8N4P3OK0rV1/Ce0XT7glqukZLq8dPPgCtVUm+TJDDSuuoLaN5290tufGj9M+b7yxi9p2Pf8MtV3QwWWtiSrPROu+4HJne36KF5XcfaSH2i5a6V5OCgBGx8aorUBU0UKJ95mY4BmHErDEUynH5bBM2p1t1tbEs9COB7yfkwWeETc25i4cCQCJZi6zNq93Zw8mcjwFpTHd5WyPBGSC2p3dMEKCBbthhAQLdsMICRbshhESLNgNIyRYsBtGSKir9Fb1PORn3DLJiz1P0n5Dw25Jpjg5QfskA7KMLt2yidq8aS5DvfDEc8723n4uGVUrXKrJxLm8VhEu1XSmeb+dP/1HZ/tPfvIj2mdVdye1tV/GCxje9LHbqa11pXs9uonjXNZKtWSpbU/Pz6jtwK5nqW1owj3+EY9nDh4dHaU2L6CQaSrJ750x4e/Z727/uLP93u8fpH2iUaJtBuzH7uyGERIs2A0jJFiwG0ZIsGA3jJBgwW4YIcGC3TBCQn2lt2oJExNHnLbJAS5foeCW69LKi+t1ru6gtkyGZ40lSHFLAEgh72yP59ztAJCb5se1Yd0Gamvvdq/lBQBDvb3UtrLVXdDxsou30j5Nab7G2vtu+nfUtvHyLdTWmHL7kWrm66F5ytcV2HbjTdR2vO8AtY0O73a2xwJuc+tXrqW2Q4Nclru4yDMEIxEuif388X9xthdGeaZfrNWdJep5Jr0ZRuixYDeMkGDBbhghwYLdMEKCBbthhIQ5Z+NFJAngaQAN/uv/XlX/UETOBfAAgHYALwK4TVV51geAWDyBju71TtvVv/pB2u+lX/yDs706w5cSWnf+jdSWq/BZ33hAMkPnendyx9FCQJ8ZPhtf8ni/xi4+G79xI1+iavKAu+ZdSxtPMklm+dJKl119FbV5Hn+7J0eHne0ScIoU8wVqS2VXUFtjK0/kmS66xyMV52O/LiAxqPcYP3ce/glPokpnW6itr99dly/byWf30+1uhapc5MrQfO7sMwA+oKqXoLY88w0iciWAPwXwVVU9D8AYgDvmsS3DMJaJOYNda5wo4xr3HwrgAwD+3m+/F8BHlsRDwzAWhfmuzx71V3AdAvA4gP0AxlX1RFLtUQC87q1hGMvOvIJdVauqugXAGgDbAFww3x2IyHYR6RGRnlyO13k3DGNpOa3ZeFUdB/AkgPcAyIrIiQm+NQD6SJ8dqrpVVbdmMnxNbMMwlpY5g11EOkUk66eoHpMAAAQxSURBVD9vBHA9gDdRC/qP+S+7HcDDS+WkYRgLZz6JMN0A7hWRKGoXhwdV9Uci8gaAB0TkSwBeBvDtuTYUBZAiuSszU7ye3NrVFzvbD+zlNbpyUwPUFonyZIzCNB+SgUH3/nIFvr2VrTxZp/2cc6mtDJ6ckowGJABd5F6+qvImr8k3cOAX1LZ7F7el4vyT2siQO1knyt1Atn0ltXke7zg4xOXNRKs7qaW9wS0NAsDEOF/G6Vc2rae2A33HqG2yyP1vW7HG2d7axMf3rT0/d7bPBCyJNmewq+ouAJc62g+g9v3dMIy3AfYLOsMICRbshhESLNgNIyRYsBtGSLBgN4yQIKq8ZtWi70xkGMBh/98OAHwtoPphfpyM+XEybzc/zlFVZ9peXYP9pB2L9Kgqr4Jofpgf5sei+mEf4w0jJFiwG0ZIWM5g37GM+56N+XEy5sfJvGP8WLbv7IZh1Bf7GG8YIcGC3TBCwrIEu4jcICK7RWSfiNy5HD74fhwSkVdF5BUR4aVBF3+/94jIkIi8NqutTUQeF5G9/l9e8nVp/bhLRPr8MXlFRHiZ3sXzY62IPCkib4jI6yLyX/32uo5JgB91HRMRSYrI8yKy0/fjj/z2c0XkOT9uviciidPasKrW9YFaWvt+ABsAJADsBLC53n74vhwC0LEM+30fgMsAvDar7c8A3Ok/vxPAny6TH3cB+Hydx6MbwGX+8yYAewBsrveYBPhR1zEBIAAy/vM4gOcAXAngQQC3+O3/B8B/Op3tLsedfRuAfap6QGt15h8AcPMy+LFsqOrTAE5dDvRm1Kr0AnWq1kv8qDuq2q+qL/nPp1CrhLQadR6TAD/qitZY9IrOyxHsqwHMLmOynJVpFcCPReRFEdm+TD6coEtV+/3nAwC6ltGXz4jILv9j/pJ/nZiNiKxHrVjKc1jGMTnFD6DOY7IUFZ3DPkF3tapeBuBDAD4tIu9bboeA2pUdtQvRcvBNABtRWxCkH8CX67VjEckA+AGAz6rqSbWh6jkmDj/qPia6gIrOjOUI9j4AswuD0cq0S42q9vl/hwD8EMtbZmtQRLoBwP87tBxOqOqgf6J5AL6FOo2JiMRRC7D7VPUhv7nuY+LyY7nGxN/3aVd0ZixHsL8AYJM/s5gAcAuAR+rthIikRaTpxHMAHwTwWnCvJeUR1Kr0AstYrfdEcPl8FHUYExER1AqWvqmqX5llquuYMD/qPSZLVtG5XjOMp8w23ojaTOd+AF9cJh82oKYE7ATwej39AHA/ah8Hy6h997oDtQUynwCwF8BPALQtkx9/C+BVALtQC7buOvhxNWof0XcBeMV/3FjvMQnwo65jAuBi1Co270LtwvIHs87Z5wHsA/B9AA2ns137uaxhhISwT9AZRmiwYDeMkGDBbhghwYLdMEKCBbthhAQLdsMICRbshhES/h/8J8qmAjupsAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#@title Qualitative comparison with ground truth {run : \"auto\"}\n",
        "datapoint_number : int = 1200 #@param{type:\"integer\"}\n",
        "\n",
        "dataset = cifar100_test if tuned_on == \"cifar100\" else imagenet1k\n",
        "try:\n",
        "    visualize(datapoint_number, dataset)\n",
        "    print(f\"Predicted: {predict(dataset[datapoint_number][0])}\")\n",
        "except IndexError:\n",
        "    print(f\"The dataset max index is {len(dataset)-1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jCqhJyWDHUgL"
      },
      "outputs": [],
      "source": [
        "#@title Quantitative result: Accuracy\n",
        "correct = 0\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    pr_label = predict(dataset[i][0], only_label=True)\n",
        "    gt_label = dataset[i][1]\n",
        "    if pr_label == gt_label:\n",
        "        correct += 1\n",
        "accuracy = round(correct*100 / len(dataset),2)\n",
        "print(f\"The accuracy of the model for the dataset {tuned_on} is: {accuracy}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m4IdUu7-7QI"
      },
      "source": [
        "##2. Code for MLP mixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqamb0Jj_ZjN"
      },
      "outputs": [],
      "source": [
        "#tentative pytorch implementation for https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_mixer.py\n",
        "#to include: apache license lol\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  MlpBlock for MixerBlock\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dim, mlp_dim):\n",
        "    super().__init__()\n",
        "    #dense implementation only has out features specified, need to check input dim\n",
        "    self.mlp1 = nn.Linear(in_features=dim, out_features = mlp_dim)\n",
        "    self.mlp2 = nn.Linear(in_features=mlp_dim, out_features = dim)\n",
        "    self.activation = F.gelu\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.mlp1(x)\n",
        "    y = self.activation(y)\n",
        "    y = self.mlp2(y)\n",
        "    return y\n",
        "\n",
        "class MixerBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Mixer block for the mlp mixer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_patches, hidden_dim,\n",
        "               tokens_mlp_dim, channels_mlp_dim):\n",
        "    super().__init__()\n",
        "    self.layer_norm = torch.nn.LayerNorm([num_patches, hidden_dim])\n",
        "    self.mlpblock_token_mixer = MlpBlock(num_patches, tokens_mlp_dim)\n",
        "    self.mlpblock_channel_mixer = MlpBlock(hidden_dim, channels_mlp_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y = self.layer_norm(x)\n",
        "    y = np.swapaxes(y,1,2)\n",
        "    y = self.mlpblock_token_mixer(y)\n",
        "    y = np.swapaxes(y,1,2)\n",
        "    x = x + y\n",
        "    y = self.layer_norm(x)\n",
        "    return x + self.mlpblock_channel_mixer(y)\n",
        "\n",
        "class MlpMixer(nn.Module):\n",
        "  \"\"\"\n",
        "  Mixer architecture\n",
        "  hidden_dim : number of hidden channels\n",
        "  img_size : the height/width of the image\n",
        "  patches_size : dimension of patches, must be a multiple of img_size\n",
        "  num_blocks : number of mixer blocks\n",
        "  hidden_dim : hidden dimensionality\n",
        "  tokens_mlp_dim : dimensionality of token mixer\n",
        "  channels_mlp_dim : dimensionality of channel mixer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, hidden_dim : int, patches_size : int,\n",
        "               num_blocks: int, tokens_mlp_dim : int,\n",
        "               channels_mlp_dim : int, num_classes: int,\n",
        "               input_channels : int, img_size :int):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_blocks = num_blocks\n",
        "    self.num_classes = num_classes\n",
        "    self.num_patches = int((img_size / patches_size)*2)\n",
        "    #Per patch fully connected\n",
        "    self.stem = nn.Conv2d(in_channels = input_channels, \n",
        "                          out_channels = self.hidden_dim,\n",
        "                          kernel_size = (patches_size,patches_size),\n",
        "                          stride = patches_size)\n",
        "    self.mixer_blocks = nn.ModuleList([]) \n",
        "    for _ in range(num_blocks):\n",
        "      self.mixer_blocks.append(MixerBlock(num_patches = self.num_patches,\n",
        "                                  hidden_dim = hidden_dim,\n",
        "                                  tokens_mlp_dim = tokens_mlp_dim,\n",
        "                                  channels_mlp_dim = channels_mlp_dim))\n",
        "    self.layer_norm = torch.nn.LayerNorm([self.num_patches, hidden_dim])\n",
        "    #What is kernel init? Does it initialize everything to 0?\n",
        "    self.fc = nn.Linear(in_features = hidden_dim, out_features = self.num_classes)\n",
        "    self.softmax = F.softmax\n",
        "\n",
        "  def forward(self, inputs, *, train):\n",
        "    del train\n",
        "    x = self.stem(inputs)\n",
        "    x = einops.rearrange(x, 'n c h w -> n h w c')\n",
        "    x = einops.rearrange(x, 'n h w c -> n (h w) c')\n",
        "    for mixer_block in self.mixer_blocks:\n",
        "      x = mixer_block(x)\n",
        "    x = self.layer_norm(x)\n",
        "    x = x.mean(dim = 1)\n",
        "    if self.num_classes:\n",
        "      #add classification layer if num classes is specified\n",
        "      x = self.softmax(self.fc(x), dim=-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMb4Dxiaxg4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e954847-d451-4da8-d90a-28500f4c490d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MlpMixer(\n",
            "  (stem): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n",
            "  (mixer_blocks): ModuleList(\n",
            "    (0): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (1): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (2): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (3): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (4): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (5): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (6): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (7): MixerBlock(\n",
            "      (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock_token_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=4, out_features=256, bias=True)\n",
            "        (mlp2): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (mlpblock_channel_mixer): MlpBlock(\n",
            "        (mlp1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (mlp2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layer_norm): LayerNorm((4, 512), eps=1e-05, elementwise_affine=True)\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n",
            "Trainable Parameters: 17.298M\n"
          ]
        }
      ],
      "source": [
        "student = MlpMixer(hidden_dim = 512,\n",
        "               patches_size = 16,\n",
        "               num_blocks = 8,\n",
        "               tokens_mlp_dim = 256,\n",
        "               channels_mlp_dim = 2048,\n",
        "               num_classes = 100,\n",
        "               input_channels = 3,\n",
        "               img_size = 32)\n",
        "print(student)\n",
        "parameters = filter(lambda p: p.requires_grad, student.parameters())\n",
        "parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "print('Trainable Parameters: %.3fM' % parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiTeILzTzowd",
        "outputId": "eb9783ca-c112-4831-bffd-623aa53b264a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 32, 32])\n",
            "tensor(1., grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "t = torchvision.transforms.functional.pil_to_tensor(dataset[0][0]).float().unsqueeze(0)\n",
        "\n",
        "print(t.shape)\n",
        "\n",
        "pred = student(t,train=False)\n",
        "\n",
        "print(pred.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzwRW42KdMsj"
      },
      "source": [
        "##3. Distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Computing labels\n"
      ],
      "metadata": {
        "id": "Td1p_uw8x7Pi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QryLx7P6dckq"
      },
      "source": [
        "First of all we want to compute once and for all the teacher scores of the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAm8oZ1AgTIO",
        "outputId": "759ce518-372d-47e9-d4ce-3a5b7009ac6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "cifar100_train_tensors = torchvision.datasets.CIFAR100(root = \".\", train = True, download = True, transform=torchvision.transforms.functional.pil_to_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS1uIiojdb3D"
      },
      "outputs": [],
      "source": [
        "def compute_scores(dataset : torch.utils.data.Dataset,\n",
        "                   teacher : torch.nn.Module, \n",
        "                   teacher_extractor : Any, # a feature extractor\n",
        "                   num_classes : int) -> torch.Tensor:\n",
        "    teacher_label = torch.zeros(len(dataset), num_classes)\n",
        "    for i,(image,_) in tqdm(enumerate(dataset)):\n",
        "        features = teacher_extractor(image,return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            label = teacher(**features).logits\n",
        "        teacher_label[i,:] = label\n",
        "    return teacher_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU8afQ8WmfgO",
        "outputId": "b5a6ee6e-c9be-41e9-9902-42ef9b1a411d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50000it [17:30, 47.60it/s]\n"
          ]
        }
      ],
      "source": [
        "#tl = compute_scores(cifar100_train_tensors,transformer_model,feature_extractor,100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating custom datasets"
      ],
      "metadata": {
        "id": "jOcxWqXqzGXK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpwMnJ1j6ODN"
      },
      "outputs": [],
      "source": [
        "class DistillDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A dataset for the distillation training.\n",
        "    This dataset wraps the original dataset toghether with the teacher ground truth.\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 original_dataset : torch.utils.data.Dataset,\n",
        "                 teacher_label_path : Path = None,\n",
        "                 teacher_model : torch.nn.Module = None,\n",
        "                 teacher_extractor : Any = None,\n",
        "                 save : bool = False\n",
        "                 ):\n",
        "        assert(teacher_label_path is not None or teacher_model is not None)\n",
        "        super().__init__()\n",
        "        self.dataset = original_dataset\n",
        "        try:\n",
        "            with open(teacher_label_path, \"rb\") as dl:\n",
        "                self.teacher_label = torch.load(dl)\n",
        "        except:\n",
        "            self.teacher_label = compute_scores(self.dataset, \n",
        "                                                teacher_model,\n",
        "                                                teacher_extractor,\n",
        "                                                len(self.dataset.classes)\n",
        "                                                )\n",
        "            if save:\n",
        "                #create directory if it does not exist\n",
        "                if not os.path.exists(teacher_label_path):\n",
        "                   os.mkdirs(teacher_label_path)\n",
        "\n",
        "                with open(teacher_label_path, \"wb\") as tl:\n",
        "                    torch.save(self.teacher_label,tl)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index][0], self.dataset[index][1] , self.teacher_label[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3gGWUDJxw9H"
      },
      "outputs": [],
      "source": [
        "dataset = DistillDataset(cifar100_train_tensors, \n",
        "                         \"teacher_labels/cifar100.pt\"\n",
        "                         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZSrAlsIyF3l"
      },
      "outputs": [],
      "source": [
        "!wget --user Ale99 --password Vitmlpmixer99 https://huggingface.co/datasets/imagenet-1k/resolve/main/data/val_images.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OBoo-ad6082"
      },
      "outputs": [],
      "source": [
        "!mkdir imagenet_val\n",
        "!tar -xvf val_images.tar.gz -C imagenet_val"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Training the student"
      ],
      "metadata": {
        "id": "89YUomqmx-pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: reorganize\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "\n",
        "\n",
        "#pinnare memoria?\n",
        "train_dl = DataLoader(dataset, batch_size = 512, shuffle = True, num_workers = 0,\n",
        "                      pin_memory = False)\n",
        "\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "optimizer = SGD(student.parameters(), lr=0.0025, momentum=0.75)\n",
        "\n",
        "for i in range(5):\n",
        "  for iteration, data in tqdm(enumerate(train_dl)):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    teacher_gt = F.softmax(data[2], dim=-1)\n",
        "    hard_gt = (teacher_gt.argmax(-1)).float()\n",
        "    preds = student(data[0].float(), train=False)\n",
        "    hard_labels = (preds.argmax(-1)).float()\n",
        "\n",
        "    loss = criterion1(preds, teacher_gt) \n",
        "\n",
        "    if iteration % 20 == 0:\n",
        "      print(\"Current batch loss: \",loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBULRavFyNgK",
        "outputId": "34b30662-50f5-4191-a538-c2697a0d2837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.6001, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [02:26,  6.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5986, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [04:43,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5787, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [06:57,  6.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5856, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80it [09:13,  6.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5838, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "98it [11:12,  6.86s/it]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5910, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [02:14,  6.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5912, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [04:28,  6.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5804, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [06:43,  6.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5810, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80it [08:56,  6.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5958, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "98it [10:58,  6.72s/it]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5894, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [02:21,  7.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5964, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [04:43,  6.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.6022, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [07:03,  6.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5976, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80it [09:24,  6.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5953, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "98it [11:28,  7.03s/it]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5923, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [02:24,  7.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5854, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [04:45,  7.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5894, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [07:07,  7.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5984, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80it [09:26,  6.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5894, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "98it [11:31,  7.05s/it]\n",
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5894, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [02:21,  7.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5914, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "40it [04:39,  6.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5890, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "60it [07:00,  7.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5906, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "80it [09:20,  6.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current batch loss:  tensor(4.5958, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "98it [11:26,  7.00s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}