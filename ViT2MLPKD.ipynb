{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ES_z2lrFzS4Q"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge distillation\n",
        "In this notebook, we will try to distill the knowledge of a Vision Transformer into a MLP Mixer."
      ],
      "metadata": {
        "id": "dL7l7ZJI61rQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##0. Downloads and imports"
      ],
      "metadata": {
        "id": "ES_z2lrFzS4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloads\n",
        "!pip install -q transformers datasets[vision] pytorch-lightning\n",
        "!pip install einops"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Ol5yDer7LaO",
        "outputId": "9e4c6951-71d5-41ec-d6bf-365aba06e3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.8 MB 13.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 451 kB 62.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 798 kB 29.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 61.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 512 kB 55.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 19.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 51.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 53.2 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 492 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "\n",
        "# general imports\n",
        "from typing import *\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pytorch imports\n",
        "import torchvision\n",
        "import transformers\n",
        "#from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device : str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "WumRkwLd7SKG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Move to the project folder\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "%cd /content/drive/MyDrive\n",
        "\n",
        "if not os.path.exists(\"ViT2MLP\"):\n",
        "  os.mkdir(\"ViT2MLP\")\n",
        "\n",
        "%cd ViT2MLP"
      ],
      "metadata": {
        "id": "GdzEa5BqwFOy",
        "cellView": "form",
        "outputId": "b3f61576-3dfb-402b-a548-ac7c568a41b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ViT2MLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the datasets\n",
        "# TODO need to download it\n",
        "#imagenet = torchvision.datasets.ImageNet(root = \".\",split = \"val\")\n",
        "\n",
        "cifar100_test = torchvision.datasets.CIFAR100(root = \".\", train = False, download = True)\n",
        "cifar100_train = torchvision.datasets.CIFAR100(root = \".\", train = True, download = True)"
      ],
      "metadata": {
        "id": "_yAlyRDW8fwM",
        "outputId": "fc08960b-2ba2-4c29-e7e6-9dc7fa22fb39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualization function\n",
        "def visualize(datapoint : int, dataset : torchvision.datasets):\n",
        "    image, label = dataset[datapoint]\n",
        "    plt.title(f\"Ground truth: {dataset.classes[label]}\")\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MLtary4p_Z_B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Code for the pre-trained transformer"
      ],
      "metadata": {
        "id": "AjYPKIo8zW_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- [main transformer docs](https:huggingface.co/docs/transformers/index) \n",
        "- [ViT Docs](https://huggingface.co/docs/transformers/model_doc/vit)\n",
        "- [lighting finetuning example](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer)"
      ],
      "metadata": {
        "id": "AcazhnqFv0Lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of transformer model\n",
        "\n",
        "Load ViT finetuned on Imagenet1k or CIFAR100. To load the non fine-tuned version use **\"google/vit-base-patch16-224-im21k\"**"
      ],
      "metadata": {
        "id": "MKpdGxL59-Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load the model\n",
        "cifar100_vit = \"Ahmed9275/Vit-Cifar100\"\n",
        "imagenet1k_vit = \"google/vit-base-patch16-224\"\n",
        "tuned_on : str = \"cifar100\" #@param['cifar100', 'imagenet1k']\n",
        "if tuned_on == \"cifar100\":\n",
        "    model_str = cifar100_vit\n",
        "else:\n",
        "    model_str = imagenet1k_vit\n",
        "\n",
        "\n",
        "feature_extractor = transformers.ViTFeatureExtractor.from_pretrained(model_str)\n",
        "\n",
        "model = transformers.ViTForImageClassification.from_pretrained(model_str).to(device)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WsSKp30aAMah"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function to predict\n",
        "def predict(image, only_label : bool = False) -> str:\n",
        "    inputs = feature_extractor(image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    # model predicts one of the 1000 ImageNet classes\n",
        "    predicted_label : int = logits.argmax(-1).item()\n",
        "    if only_label:\n",
        "        return predicted_label\n",
        "    return model.config.id2label[predicted_label]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qW2bXu6dAYls"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Qualitative comparison with ground truth {run : \"auto\"}\n",
        "datapoint_number : int = 1200 #@param{type:\"integer\"}\n",
        "\n",
        "dataset = cifar100_test if tuned_on == \"cifar100\" else imagenet1k\n",
        "try:\n",
        "    visualize(datapoint_number, dataset)\n",
        "    print(f\"Predicted: {predict(dataset[datapoint_number][0])}\")\n",
        "except IndexError:\n",
        "    print(f\"The dataset max index is {len(dataset)-1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "RBCle0TPyy2b",
        "outputId": "1c1862a3-09d1-453c-9150-48c30a001335",
        "cellView": "form"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: lion\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de5Bc9XXnv6df09PdM9Pz0mj0QkjIgHBAgNBiAw52wMEQL/auywtOsWyKWN6Nvbuu2JWl7FRCst4Nycav2nK8kWNikmAwjnEBDkmMWQgG28DwkHjq/RiN5qV5d/f09OOe/aOv1iPV73tnpJnpEdzzqeqant/p373n/vqee7t/3z7nJ6oKwzDe+USW2wHDMOqDBbthhAQLdsMICRbshhESLNgNIyRYsBtGSLBgDxEisl5EVERidd7vUyLy22fY91oROTrr/9dF5NpFcy5EWLAvMiJyi4g8JyJ5ERnyn/+OiMhy+zYXInJIRK5b4DbuEpG/WyyfTkVVL1LVp5Zq++9kLNgXERH5HICvA/hfAFYC6ALwHwFcBSBB+kTr5uACqfcnAmORUVV7LMIDQAuAPIB/O8frvgPgmwAe819/HYALATwFYBzA6wD+9azXPwXgt2f9/x8APDPrf0XtgrLX7/8NAOLbogD+HMBxAAcAfNp/fczh198C8ABMA8gB+D0A6/3X3wHgCICnAVwL4OgpfQ/5x3EDgBKAsr+NnbOO4b8DeBbAFIAfA+iY57ietL8T+/KfNwD4GoBj/uNrABpm9wPwOQBDAPoB/NZynyfL+bA7++LxHtROvofn8dpPAPgfAJoAPAfgUdQCYAWA/wzgPhE5/zT2/RsArgBwMYCPA/h1v/2Tvu1SAFsBfIxtQFVvQy2gP6yqGVX9s1nmX0XtgvTrzs6/3MY/AfifAL7nb+OSWeZPAPgt1I4xAeDzJwwisktEPjGP4zyVLwK4EsAWAJcA2Abg92fZV6J2EV6N2gXrGyLSegb7eUdgwb54dAA4rqqVEw0i8jMRGReRaRF536zXPqyqz6qqh9qJmgFwt6qWVPX/AvgRgFtPY993q+q4qh4B8KS/TaAW+F9T1V5VHQXwJ2d4bHepal5Vp8+wPwD8taru8bfx4CwfoaoXq+p3z2Cbvwngj1V1SFWHAfwRgNtm2cu+vayqj6H2aeN0LqLvKCzYF48RAB2zv9eq6ntVNevbZo9176znqwD0+oF/gsOo3Y3my8Cs5wXULh7/f9unbPdM6J37JXPCfFwIq3DyMR32204wMvviu4j7fVtiwb54/BzADICb5/Ha2amGxwCsFZHZ78U6AH3+8zyA1CzbytPwqR/A2lO2O1+/WPtJ/vgTjJ3z2MZScAzAObP+X+e3GQ4s2BcJVR1H7WPkX4jIx0SkSUQiIrIFQDqg63Oo3XF+T0Tivob8YQAP+PZXAPwbEUmJyHmoffecLw8C+C8issb/rnrnHK8fBLBhjtfsAZAUkZtEJI7ad+SGU7ax/pSL11JxP4DfF5FOEekA8AcAlkz2e7tjwb6I+JNav4vaTPag//hLAP8NwM9InxJqwf0h1GbN/wLAv1fVt/yXfBW1Ge5BAPcCuO80XPoWgH8GsBPASwAemuP1f4Ja8IyLyOddL1DVCQC/A+CvUPv0kUdt1vsE3/f/jojIS/Nx0v+hzG/O57Wn8CUAPQB2AXgVtWP80hlsJxSckGgMw3iHY3d2wwgJFuyGERIs2A0jJFiwG0ZIqGtiQzKZ0HS60W0MuOxUvarbcNLvUE7ZXJRvMBZx5qQAAJpSHdyPint/0RgfxiA/olGeAxML2GaQquV5ZWd7cWKI9ol4JW6LcB+np/kP6qqVirO9XCHvJYCgyeJolCcNxgPGUZgtEqd9Ksq3F4k1UFuZHHPNkaCkR/dxe+R8q/nh9nEyN4np4rRzZwsKdhG5AbUsryiAv1LVu4Nen0434kM3XeneVpIPxlR+ytleKRdon1Qzl7bbm/mP0z5w6SepbXx40tme7VhB+zS2cD+yLU3U1tbJt5mI84vVdH7Y2b770f9N+zSU+A/kUinu4+s736C23Oigs71vZIL2KZf4yZ1t4sHZ1cp/FNeQbnO2a5qP77DHfz6faj+X2o4NH6e2aCLgglR1XySmJvK0T1NLu7P9vn/gyuwZf4z3fzn1DdT04c0AbhWRzWe6PcMwlpaFfGffBmCfqh7wfxjyAOb3U1HDMJaBhQT7apycIHEUjuQNEdkuIj0i0lOc4d8NDcNYWpZ8Nl5Vd6jqVlXdmmzg3zUNw1haFhLsfTg5o2oNfpmpZRjGWcZCZuNfALBJRM5FLchvQa0aCSUSF2Q63dJbseSecQeAZMU9U+9FiIwHIJvmM7TrW6+htpH+EWqLN7n9iCT5LHIiQKpBlM8wj425Z/4BYHRogNoqx152b+/obtpn7aoWaus9wvc1NDBKbYeH3bPusTi/v1y+qZvaygHS1av7uY+XX+ieWY8qV3IqM1xBGR8fp7bJMX7uTFdmqK0l0+ze3hSfjS+U3DP4lTKX/8442FW1IiKfQS2rKgrgHlV9/Uy3ZxjG0rIgnd0v9fPYIvliGMYSYj+XNYyQYMFuGCHBgt0wQoIFu2GEhPou8KcRRKruH9aszq51tgPAcMKdYJBo4BJJ08w51LbrF85ycACANWt5osOVV73f2d7ZwYu2siw0ANAi/0VhbopLb7mjB7jt8PNuQ5VLP5MTPHutmOeJK9EElxwzTW7pc9u7eT3LZJUfc6qVZyMem+KZdL2D7m1e+C633AUAkTI/rqkCl+wSCS6lBiQPAlX3/pqyPAmprG6JLajMp93ZDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXWfjAUUk4p55bEi4ywcBQGzSPVt86BBP7pg69Ca1jY/wZIajR/g2X+75qbP9vPO3ONsB4MprPkRte97YSW0DATPul23spLZVKffM9Jv7eZJGazZJbTHhisHIFJ/Fv/B896z7+Zu5SvLE069QW3eCJ8Jc//73Utvf3e9eQbstyxOUWtp5ySqNcFUg1ZyitskxXgOwf8BdFmwmz4+5OOMe+0pAHTy7sxtGSLBgN4yQYMFuGCHBgt0wQoIFu2GEBAt2wwgJdZXePE8xVXAnhkyXj9J+hSl3MkbfMZ6k4eX4UkLlKk9O8apcapqadCdBvPryU7QP8x0Atm67itqquX5qaxAuHa5a4U4OOpQIWE5K+Xgc6eN15mIBWRedWfeKJYjx5KVLL+ZJMkf6+GorqRSvRXj+ho1ug/DzI97IpchknO9r9DivhTccYCuTcy6X5+dicTrnbPeqPCnI7uyGERIs2A0jJFiwG0ZIsGA3jJBgwW4YIcGC3TBCQl2lt3IFOD7oljwaM1wKiUXcGUrZVr5c0OgUz/JScEkjnuCZSw0N7iykIOnqeP8+atu/2700EQA0R4vUtq6L+5hOu7MKN67n2VrJOC+QVpzmteuyLbxG2hhZuujQT1+ifVZ1clkuE+f13R55ktTdA9Aac2eBNQVk0UXifHz7A2rQlUs8CzDbmqW26Rn3+M+UeE0+EbePkQi/fy8o2EXkEIApAFUAFVXdupDtGYaxdCzGnf39qsp/8WAYxlmBfWc3jJCw0GBXAD8WkRdFZLvrBSKyXUR6RKSnNMO/KxuGsbQs9GP81araJyIrADwuIm+p6tOzX6CqOwDsAICW1iyfhTMMY0lZ0J1dVfv8v0MAfghg22I4ZRjG4nPGd3YRSQOIqOqU//yDAP44qI9WFaW8WwpRspwNACDq/kDQ1sELA8bJMlMA0Jd3y0IAUK5yySuVchfFrMzwTCOW0QQA0wEZcas7+VuTbODLE3lkmae153DpLX+cy5TvWssloz3HpqjtQL87W66fyEwAkMlyee2C1Tzb7GiZZ6lVRtzHlmnhHzIbm7nc2DfDl41Kda6htqEhntU5MTHmbFeuDmJszC3LVQKy3hbyMb4LwA9F5MR2vquq/7SA7RmGsYSccbCr6gEAlyyiL4ZhLCEmvRlGSLBgN4yQYMFuGCHBgt0wQkJds94UHmaq7qyh0UEu/5xz7jpne1S4VNOQ4odWES6vVTxu89QtyUiUXzPZmlwAUJjkhSPLTdz/qHCpqVxxSy9dq0gBSABe0V28EAA2ruf9Csqzww4MuGW5ay67gvbpSHP5tX0Vz4j7lRiX0V4eHnS2Z1tbaJ9khsuDGOQZjtGAYpS5PJcpp4vu86opHZAV2er2I3qEn4t2ZzeMkGDBbhghwYLdMEKCBbthhAQLdsMICfWdja96KE24ZyUvuvRC2i9fcM9ax6M8ySTWxBNhUik+i5xQPqM6POwuyJNO8tnxTENA7bECn6mPkBpjAFCY5Ak02dXu/VXBZ6xbuniSjCrv9+5UF7UNF/c720eGh2ifbIKPVanM30/1+Ex3qtH9fsYT/D1ra+LHXCy6k1YAoORxNaEC/l7HEu5j62jniV4VcSdDRaNcSbA7u2GEBAt2wwgJFuyGERIs2A0jJFiwG0ZIsGA3jJBQV+mtUq1gZNwtXQwP8qSQZNwtTTQFLBdUnua1uNrb3bXkACDlceliw5pOZ/uqFStpn4NHuIwzMNhHbZOT3P9K2b0cFgDEYm7/CwHLOMWEX/MlxiWveJTXwltBpM9ShMulxQrf3vFh7n8iYPmq1ozbj5mAfTUE+Igyl976R/hyTUH31WLRnXwVVL8wRpLA/DJxp+mBYRjvKCzYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXaW3WCyC9g53FlJ3mktN565ucrZnA6SfhirPMrr+8rXUtmoFz7xq73Bnh43nuB/39O2ltsmA5Z8mpoKuw24JEADGyFJOEuHyVCJgHKMNPPsuGeEyz5oV7vppowFjtWfPPmp7dIDLUJ0BNeO2bHC/n7GAuoGRGJc2V2a5/72jvJaf5/F6iaMTbtk528LPD89zS7pBWYpz3tlF5B4RGRKR12a1tYnI4yKy1//LK+MZhnFWMJ+P8d8BcMMpbXcCeEJVNwF4wv/fMIyzmDmD3V9v/dQlOW8GcK///F4AH1lkvwzDWGTO9Dt7l6r2+88HUFvR1YmIbAewHQASCf69xTCMpWXBs/FamxGgswKqukNVt6rq1njAb5gNw1hazjTYB0WkGwD8v7ywmGEYZwVn+jH+EQC3A7jb//vwfDp1pRvw+fee57StaHPLawDQmHS7KRLgfoRLRhqQGZTL8eV9BodYVhO/ZjY1cKlmZoYvNTUxxYsojo3x7Ko43IUUkwFFNqMp7n/zSi7zdTVzmVIT7vdzZu9B2qdxgI/VdICU2tbKCzN2dbiXeSoHLMvFRWCgo5mP43SRZzFWK3yMp4tuXyan+Psci7rPfU/5GM5HersfwM8BnC8iR0XkDtSC/HoR2QvgOv9/wzDOYua8s6vqrcT0a4vsi2EYS4j9XNYwQoIFu2GEBAt2wwgJFuyGERLqmvXWkGzEeZvca7olWwKkobj7msTze4BYnP9aTz2endRa5oUN9+w67GznQh6QjvL1v4IKNg4GZFANDHJJJp1yH3ckQF6LebyoZJCUM36cF1+czrmlw4O9vE9b92pqe1ec+9EeUHg0HnVLqU2tvOhole8KmQw/r4aHhqmtWOHnQYScB4VSnvaJN7j9WJD0ZhjGOwMLdsMICRbshhESLNgNIyRYsBtGSLBgN4yQUFfpTRFBkWSjTY1yyaucc0sQE2N8fTgFlyCas7yg4MZz3QUxAaCry53J1dfLZbKYcvmkrZH7eHSYy2sHB3iGYHd3s7O9oRQgASZ4nYFyjhd6lEb3vgAg3UzGapCPVT7uzlADgGijO5sPANq6eAHRePSosz3RmKF9KlUupiYCbo/pgMzNwjg/V9eu2eD2I0AGLheIbSEFJw3DeGdgwW4YIcGC3TBCggW7YYQEC3bDCAl1nY2vlEo43uuu0xWQe4CpfMHZPl3gdcRWdPKZ0WScz+yOj/C6cBHP7Uc1qD5aC0+c6Grhw390NGCm/jif2c3l3DXjvKDCaspnwb0K9yPG80/gzbh3uO1SPnNeTq6hNknwGfJVrXyMIwX3GI9N8Bp/aeWKTDLBz52VHfycGxg/demFX8Im0Fs7eG293sNulSEoOczu7IYREizYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCfaW3ahWjY24JohJQo6u5xZ1w0dLNlyZKxLgI4QUJfcplnKjn3mY+z5M7PI/vK5Piw98UIDUVeG4K9h457myPKL+uXwB+zM3NfBxj+QlqSyTcyUar16ykff7l2R5qG+wfobYrLnEnkgDAOrK/aoUv81WY4vKrF+H1+lZkuCzX3szlzdyke6nEzq522icecUubQfUQ57P80z0iMiQir81qu0tE+kTkFf9x41zbMQxjeZnPx/jvALjB0f5VVd3iPx5bXLcMw1hs5gx2VX0aAP/5j2EYbwsWMkH3GRHZ5X/Mb2UvEpHtItIjIj1T0zwZ3zCMpeVMg/2bADYC2AKgH8CX2QtVdYeqblXVrU2NvEKMYRhLyxkFu6oOqmpVVT0A3wKwbXHdMgxjsTkj6U1EulW13//3owBeC3r9CTzPQ45kqpVK/CO+xEiNNNYOIBLlMkiVq3zI5bkfkaJbkskFZFCVily6SkT58KcbuG1qkmfZ7dzt9jHZEPCpKiCTa+O6LmrL5/hyR+/afL6z3RN+f1m3li//NJPnxywBWYzTJbf06REZFQCKuSDpjZrQ0sjTADPNvObdxMigs328z53ZBgCi/NxnzBnsInI/gGsBdIjIUQB/COBaEdmCWkbdIQCfOu09G4ZRV+YMdlW91dH87SXwxTCMJcR+LmsYIcGC3TBCggW7YYQEC3bDCAl1z3obn3JnSnlVLoWMTLgLPXoyQPuce846amtr5lLNTI7/Mjhecvs+Oc6z3ope0BAHyHJxLq0MjfIlpYbH3VJTVxvPh3prH5d4yiWeHVaZ4RJVpoVleQVkI3rcx42beGbbRIAsl0i6dVat8NTBfI7Lr0GKlxcgszZFeWZhUdznSKzEq4Sm4u6imJEAadPu7IYREizYDSMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXaW3crmCvn63tBWJ8OtOosEtlVUCpJq+wTFqy+e4DFLJ837ZhFuuKZe59FOqBlxPlfvfGFBwUtniYAAmy265JjXD5SRvjKcBqvLMtsYGrkO19JECkcrlpPFxXsDyiksvpLbRMf6eFYl02JZ1FzEFgIHhXmqbDMg4zDRnqW1zZwe1Hai43+tVnbzg5HTZ/X6+ECDZ2p3dMEKCBbthhAQLdsMICRbshhESLNgNIyTUdTZeVTFDZotbmnhySnPaPXteBV+KZ2qKJ6fkJvnMdDbJZzPzVfdsfICQgOoMTyQpF/nMdCbBjy2d4G/bRNE9s54j7QDgxfnsfkOR+y8RrmocJapLIqC2XqHAl8rad5irAi1ZvrTSW3uPONsvueg82scLCIvR3CS1dXfzen3XXPevqC15zkXO9pefeYX2Gend72xviPP3xO7shhESLNgNIyRYsBtGSLBgN4yQYMFuGCHBgt0wQsJ8VoRZC+BvAHShVkBsh6p+XUTaAHwPwHrUVoX5uKryjAQAVVXkqJTD66qVSu5aZ6k0T2aYnOTSGxeagDSpBwYA+Yhbeks1ctnQ8wLqmQX4UVUuQ3U286WcJklptekql/lUAxJhJt31/wAgKtyPouce/0w6RftEhMueBwe45LU6wsd/aNx93LuP8FM1EZCgNFHg7+dEnsuU2W6eCLPmwpXO9o3vvoX2efYfe5ztjY8+QfvM585eAfA5Vd0M4EoAnxaRzQDuBPCEqm4C8IT/v2EYZylzBruq9qvqS/7zKQBvAlgN4GYA9/ovuxfAR5bKScMwFs5pfWcXkfUALgXwHICuWSu5DqD2Md8wjLOUeQe7iGQA/ADAZ1X1pC9QWqum4PwKKiLbRaRHRHpmKvx7qGEYS8u8gl1E4qgF+n2q+pDfPCgi3b69G8CQq6+q7lDVraq6tSFmk/+GsVzMGX0iIqgt0fymqn5llukRALf7z28H8PDiu2cYxmIxn6y3qwDcBuBVETmRhvMFAHcDeFBE7gBwGMDH59pQyVP0Ft3aUI5kSQFAssEt8cSq/c52AGhu5FljjUkuGa1duYba2pszzvbCyBTto8olxdYkz1BqDHhnNCDbrEC+Ko3kuKw1Mc2XQioFLJOUjPN6bLGye3+5AOkqSJYrV7ltaJiPf2drm7M9WubyWraLTz9NvNVHbYd7j1Hb1ASX5YZed8toU1hB+2RWuMcjEuf37zmDXVWfAcBG5tfm6m8YxtmBfYk2jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1LTjpCTDd4M5Cqma4VOaRrLJ9h8Zpn9VVLq1sbuHL9MQamqittcNt62hJ0z7V6DpqKxR53tuxw3upbXWAvLKiq9vZ/sQLe2if4YBCj+pxH0fyXHprSbvHpFTkWXRV5cc1wxVAVAN+mdm1sdPZ3tnCT/13X76B2kYDxmPo8AC1/ewXu6ntvE0bne3F3FO0z8ED7kzQ/DiPCbuzG0ZIsGA3jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1ld7iiRhWrnMX1ysErM0WK7mvSdGA7K/JADmmuZmvDda+YjW1rVztlgc7mngxwcYO9/ECQKqzndomCtdRG8r82CKt5zjbd33qi7RPX9Et4wDAZI7bZgo8k4t5mIxxifX4OM9eSycD9lXhmXSDQ+5zREs8i25tP19X7sM3XkNt33voZWqLBhQyfev1fc72LRcFZDeW3GPlBUiDdmc3jJBgwW4YIcGC3TBCggW7YYQEC3bDCAl1nY2HCDTqnmE8PsJn4yvj7iyI7gxf9meqzJc0Wr9pPbU1JhupLT/tnulsbeF99r+xk9o0FrAUUtad0AIA6fQqaps41uvuk2qlfVqzPHkiHuXH1tXOVYjuVnciTCbJ3zMVfu9pTPBTNRHnSU+9R9zjoRE+a31w32Fqa1/F68Ldcuv11PbCUz+ltpmie2mrUoEnUUUayLkT4WNod3bDCAkW7IYREizYDSMkWLAbRkiwYDeMkGDBbhghYU7pTUTWAvgb1JZkVgA7VPXrInIXgE8COJE18AVVfSxoW17VQ37CXYMsGQtYCqnDLf+kAxJhMlWeOJFpbaa2fG6E2vYddydc7D/CkztKyiWvxo711FYZ5ckuTRl3HT8AKFXcdeG2XHE17bO5cBm1tTXxpbI2XXQetSVibv9TGfcSWgDQ0MilSKGLEgGVMk+E+evvfNfZ/vzzz9M+5YCCd41pLh1e3cZrEa7q4udcpeju1z/K6/VVYm5ZToWfi/PR2SsAPqeqL4lIE4AXReRx3/ZVVf3zeWzDMIxlZj5rvfUD6PefT4nImwB4HqhhGGclp/WdXUTWA7gUwHN+02dEZJeI3CMi/POqYRjLzryDXUQyAH4A4LOqOgngmwA2AtiC2p3/y6TfdhHpEZGecpl/1zQMY2mZV7CLSBy1QL9PVR8CAFUdVNWqqnoAvgVgm6uvqu5Q1a2qujUe52uEG4axtMwZ7CIiAL4N4E1V/cqs9tmZGh8F8Nriu2cYxmIxn9n4qwDcBuBVEXnFb/sCgFtFZAtqctwhAJ+aa0PJaAIXZN1ze41rLuROxtzyTy7PM9skxbPo+ia5VNPW9h5qa9ywxtneRJanAoBkQGbeyi6+1FQ0xj8FFae5rBhRt+TVPTZI+wyO9lNbCrwGnSjPHCsW3P3K4OOBqSFqyjTzKaGHH/1nahs47q7VlkhyCXAkoO7eyDhf8qr3rUPUVhH+fj76lLt23Z5D/H15302fdbZXPS5Rzmc2/hnAKXIGauqGYZxd2C/oDCMkWLAbRkiwYDeMkGDBbhghwYLdMEJCXQtOZprb8N4P3OK0rV1/Ce0XT7glqukZLq8dPPgCtVUm+TJDDSuuoLaN5290tufGj9M+b7yxi9p2Pf8MtV3QwWWtiSrPROu+4HJne36KF5XcfaSH2i5a6V5OCgBGx8aorUBU0UKJ95mY4BmHErDEUynH5bBM2p1t1tbEs9COB7yfkwWeETc25i4cCQCJZi6zNq93Zw8mcjwFpTHd5WyPBGSC2p3dMEKCBbthhAQLdsMICRbshhESLNgNIyRYsBtGSKir9Fb1PORn3DLJiz1P0n5Dw25Jpjg5QfskA7KMLt2yidq8aS5DvfDEc8723n4uGVUrXKrJxLm8VhEu1XSmeb+dP/1HZ/tPfvIj2mdVdye1tV/GCxje9LHbqa11pXs9uonjXNZKtWSpbU/Pz6jtwK5nqW1owj3+EY9nDh4dHaU2L6CQaSrJ750x4e/Z727/uLP93u8fpH2iUaJtBuzH7uyGERIs2A0jJFiwG0ZIsGA3jJBgwW4YIcGC3TBCQn2lt2oJExNHnLbJAS5foeCW69LKi+t1ru6gtkyGZ40lSHFLAEgh72yP59ztAJCb5se1Yd0Gamvvdq/lBQBDvb3UtrLVXdDxsou30j5Nab7G2vtu+nfUtvHyLdTWmHL7kWrm66F5ytcV2HbjTdR2vO8AtY0O73a2xwJuc+tXrqW2Q4Nclru4yDMEIxEuif388X9xthdGeaZfrNWdJep5Jr0ZRuixYDeMkGDBbhghwYLdMEKCBbthhIQ5Z+NFJAngaQAN/uv/XlX/UETOBfAAgHYALwK4TVV51geAWDyBju71TtvVv/pB2u+lX/yDs706w5cSWnf+jdSWq/BZ33hAMkPnendyx9FCQJ8ZPhtf8ni/xi4+G79xI1+iavKAu+ZdSxtPMklm+dJKl119FbV5Hn+7J0eHne0ScIoU8wVqS2VXUFtjK0/kmS66xyMV52O/LiAxqPcYP3ce/glPokpnW6itr99dly/byWf30+1uhapc5MrQfO7sMwA+oKqXoLY88w0iciWAPwXwVVU9D8AYgDvmsS3DMJaJOYNda5wo4xr3HwrgAwD+3m+/F8BHlsRDwzAWhfmuzx71V3AdAvA4gP0AxlX1RFLtUQC87q1hGMvOvIJdVauqugXAGgDbAFww3x2IyHYR6RGRnlyO13k3DGNpOa3ZeFUdB/AkgPcAyIrIiQm+NQD6SJ8dqrpVVbdmMnxNbMMwlpY5g11EOkUk66eoHpMAAAQxSURBVD9vBHA9gDdRC/qP+S+7HcDDS+WkYRgLZz6JMN0A7hWRKGoXhwdV9Uci8gaAB0TkSwBeBvDtuTYUBZAiuSszU7ye3NrVFzvbD+zlNbpyUwPUFonyZIzCNB+SgUH3/nIFvr2VrTxZp/2cc6mtDJ6ckowGJABd5F6+qvImr8k3cOAX1LZ7F7el4vyT2siQO1knyt1Atn0ltXke7zg4xOXNRKs7qaW9wS0NAsDEOF/G6Vc2rae2A33HqG2yyP1vW7HG2d7axMf3rT0/d7bPBCyJNmewq+ouAJc62g+g9v3dMIy3AfYLOsMICRbshhESLNgNIyRYsBtGSLBgN4yQIKq8ZtWi70xkGMBh/98OAHwtoPphfpyM+XEybzc/zlFVZ9peXYP9pB2L9Kgqr4Jofpgf5sei+mEf4w0jJFiwG0ZIWM5g37GM+56N+XEy5sfJvGP8WLbv7IZh1Bf7GG8YIcGC3TBCwrIEu4jcICK7RWSfiNy5HD74fhwSkVdF5BUR4aVBF3+/94jIkIi8NqutTUQeF5G9/l9e8nVp/bhLRPr8MXlFRHiZ3sXzY62IPCkib4jI6yLyX/32uo5JgB91HRMRSYrI8yKy0/fjj/z2c0XkOT9uviciidPasKrW9YFaWvt+ABsAJADsBLC53n74vhwC0LEM+30fgMsAvDar7c8A3Ok/vxPAny6TH3cB+Hydx6MbwGX+8yYAewBsrveYBPhR1zEBIAAy/vM4gOcAXAngQQC3+O3/B8B/Op3tLsedfRuAfap6QGt15h8AcPMy+LFsqOrTAE5dDvRm1Kr0AnWq1kv8qDuq2q+qL/nPp1CrhLQadR6TAD/qitZY9IrOyxHsqwHMLmOynJVpFcCPReRFEdm+TD6coEtV+/3nAwC6ltGXz4jILv9j/pJ/nZiNiKxHrVjKc1jGMTnFD6DOY7IUFZ3DPkF3tapeBuBDAD4tIu9bboeA2pUdtQvRcvBNABtRWxCkH8CX67VjEckA+AGAz6rqSbWh6jkmDj/qPia6gIrOjOUI9j4AswuD0cq0S42q9vl/hwD8EMtbZmtQRLoBwP87tBxOqOqgf6J5AL6FOo2JiMRRC7D7VPUhv7nuY+LyY7nGxN/3aVd0ZixHsL8AYJM/s5gAcAuAR+rthIikRaTpxHMAHwTwWnCvJeUR1Kr0AstYrfdEcPl8FHUYExER1AqWvqmqX5llquuYMD/qPSZLVtG5XjOMp8w23ojaTOd+AF9cJh82oKYE7ATwej39AHA/ah8Hy6h997oDtQUynwCwF8BPALQtkx9/C+BVALtQC7buOvhxNWof0XcBeMV/3FjvMQnwo65jAuBi1Co270LtwvIHs87Z5wHsA/B9AA2ns137uaxhhISwT9AZRmiwYDeMkGDBbhghwYLdMEKCBbthhAQLdsMICRbshhES/h/8J8qmAjupsAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Quantitative result: Accuracy\n",
        "correct = 0\n",
        "for i in tqdm(range(len(dataset))):\n",
        "    pr_label = predict(dataset[i][0], only_label=True)\n",
        "    gt_label = dataset[i][1]\n",
        "    if pr_label == gt_label:\n",
        "        correct += 1\n",
        "accuracy = round(correct*100 / len(dataset),2)\n",
        "print(f\"The accuracy of the model for the dataset {tuned_on} is: {accuracy}%\")"
      ],
      "metadata": {
        "id": "jCqhJyWDHUgL",
        "outputId": "d0a81117-b41c-4bb0-8b68-b5330d6bec2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [03:08<00:00, 53.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model for the dataset cifar100 is: 89.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Code for MLP mixer"
      ],
      "metadata": {
        "id": "0m4IdUu7-7QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tentative pytorch implementation for https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_mixer.py\n",
        "#to include: apache license lol\n",
        "\n",
        "class MlpBlock(nn.Module):\n",
        "\n",
        "  #TODO: implement with sequential\n",
        "  def __init__(self, mlp_dim):\n",
        "    super().__init__()\n",
        "    #dense implementation only has out features specified, need to check input dim\n",
        "    self.mlp1 = nn.Linear(in_features=mlp_dim, out_features = mlp_dim)\n",
        "    self.mlp2 = nn.Linear(in_features=mlp_dim, out_features = mlp_dim)\n",
        "    self.activation = F.gelu\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.mlp1(x)\n",
        "    y = self.activation(y)\n",
        "    y = self.mlp2(y)\n",
        "    return y\n",
        "\n",
        "class MixerBlock(nn.Module):\n",
        "\n",
        "  #TODO:??\n",
        "  def __init__(self, tokens_mlp_dim, channels_mlp_dim):\n",
        "    super().__init__()\n",
        "    self.layer_norm = torch.nn.LayerNorm([200,2])\n",
        "    self.mlpblock_token_mixer = MlpBlock(tokens_mlp_dim)\n",
        "    self.mlpblock_channel_mixer = MlpBlock(channels_mlp_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    y = self.layer_norm(x)\n",
        "    y = np.swapaxes(y,1,2)\n",
        "    y = self.mlpblock_token_mixer(y)\n",
        "    y = np.swapaxes(y,1,2)\n",
        "    x = x + y\n",
        "    y = self.layer_norm(x)\n",
        "    return x + self.mlpblock_channel_mixer(y)\n",
        "\n",
        "class MlpMixer(nn.Module):\n",
        "  \"\"\"Mixer architecture\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_dim : int, patches_size : int,\n",
        "               num_blocks: int, tokens_mlp_dim : int,\n",
        "               channels_mlp_dim : int, num_classes: int):\n",
        "    super().__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    #self.patches_size = patches_size\n",
        "    self.num_blocks = num_blocks\n",
        "    self.num_classes = num_classes\n",
        "    #per patch fully connected?\n",
        "    self.stem = nn.Conv2d(in_channels = 3, \n",
        "                          out_channels = self.hidden_dim,\n",
        "                          kernel_size = (patches_size,patches_size),\n",
        "                          stride = patches_size)\n",
        "    self.mixer_block = MixerBlock(tokens_mlp_dim, channels_mlp_dim)\n",
        "    self.layer_norm = torch.nn.LayerNorm([200,2])\n",
        "    #What is kernel init? Does it initialize everything to 0?\n",
        "    self.fc = nn.Linear(in_features = channels_mlp_dim, out_features = self.num_classes)\n",
        "\n",
        "  #what is * ?\n",
        "  def forward(self, inputs, *, train):\n",
        "    #why delete train?\n",
        "    del train\n",
        "    x = self.stem(inputs)\n",
        "    x = einops.rearrange(x, 'n h w c -> n (h w) c')\n",
        "    for _ in range(self.num_blocks):\n",
        "      x = self.mixer_block(x)\n",
        "    x = self.layer_norm(x)\n",
        "    x = x.mean(dim = 1)\n",
        "    if self.num_classes:\n",
        "      #add classification layer if num classes is specified\n",
        "      x = self.fc(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Iqamb0Jj_ZjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MlpMixer(hidden_dim = 100,\n",
        "               patches_size = 16,\n",
        "               num_blocks = 1,\n",
        "               tokens_mlp_dim = 200,\n",
        "               channels_mlp_dim = 2,\n",
        "               num_classes = 100)"
      ],
      "metadata": {
        "id": "XMb4Dxiaxg4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "t = torchvision.transforms.functional.pil_to_tensor(cifar100[0][0]).float().unsqueeze(0)\n",
        "\n",
        "net(t,train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiTeILzTzowd",
        "outputId": "fd6e9c10-c1eb-4c2e-bdf2-5232e3679bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.8227e-02,  2.4850e-01, -7.0823e-01,  8.8886e-02, -4.6152e-04,\n",
              "          3.6857e-01, -1.8585e-01, -4.2545e-01, -5.2763e-02,  2.9510e-02,\n",
              "         -2.2258e-01,  4.6731e-01,  2.2371e-01,  1.9726e-01, -3.3305e-02,\n",
              "          4.7737e-01,  3.1394e-01, -2.4279e-01, -6.9914e-01, -1.6508e-01,\n",
              "          5.9465e-01, -3.1115e-01,  7.6890e-02,  5.5939e-01,  2.3915e-01,\n",
              "         -3.9857e-01,  5.3749e-01,  2.4786e-01, -6.9891e-01, -1.3914e-01,\n",
              "         -6.5187e-01,  4.0456e-01, -4.2853e-01, -1.3425e-01,  4.6789e-01,\n",
              "         -6.4191e-01, -2.5745e-01,  6.0629e-01,  2.9037e-01, -6.9886e-01,\n",
              "         -2.6686e-01,  1.3916e-01,  6.8655e-01, -6.8173e-01,  5.8004e-01,\n",
              "         -4.0788e-02,  4.6059e-01,  3.2788e-01,  3.9768e-01, -5.9938e-01,\n",
              "         -9.1302e-02,  4.5215e-01,  5.9707e-01,  1.3698e-01,  5.5988e-01,\n",
              "          4.3595e-01, -3.9343e-02, -3.7967e-01, -5.6208e-01, -7.4624e-03,\n",
              "          5.4092e-01, -3.8611e-01, -4.3657e-01, -6.3130e-01,  5.0859e-01,\n",
              "         -2.5943e-01,  9.8461e-02, -3.2540e-01,  3.0056e-01,  4.6065e-01,\n",
              "          6.3910e-01,  6.2798e-01, -6.1148e-01, -4.3509e-01, -1.9812e-01,\n",
              "          7.0235e-01,  4.0821e-01,  1.2846e-01,  6.2010e-01,  3.2825e-01,\n",
              "          4.9844e-01, -4.9055e-01, -6.9825e-01, -3.6006e-02,  4.0774e-01,\n",
              "          2.4553e-01,  1.2937e-01, -3.2408e-01, -3.6259e-01, -5.4480e-01,\n",
              "          3.0182e-01,  4.3494e-01, -4.7824e-01,  5.4890e-01, -2.3671e-01,\n",
              "          2.9464e-01,  4.8293e-01,  6.0004e-01,  7.0910e-01, -5.0180e-01]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Distillation"
      ],
      "metadata": {
        "id": "xzwRW42KdMsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we want to compute once and for all the teacher scores of the images:"
      ],
      "metadata": {
        "id": "QryLx7P6dckq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100_train_tensors = torchvision.datasets.CIFAR100(root = \".\", train = True, download = True, transform=torchvision.transforms.functional.pil_to_tensor)"
      ],
      "metadata": {
        "id": "LAm8oZ1AgTIO",
        "outputId": "3608cdb9-1ddf-49b0-ce68-bfffcadb39c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_scores(dataset : torch.utils.data.Dataset,\n",
        "                   teacher : torch.nn.Module, \n",
        "                   teacher_extractor : Any, # a feature extractor\n",
        "                   num_classes : int) -> torch.Tensor:\n",
        "    teacher_label = torch.zeros(len(dataset), num_classes)\n",
        "    for i,(image,_) in tqdm(enumerate(dataset)):\n",
        "        features = teacher_extractor(image,return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            label = teacher(**features).logits\n",
        "        teacher_label[i,:] = label\n",
        "    return teacher_label"
      ],
      "metadata": {
        "id": "oS1uIiojdb3D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl = compute_scores(cifar100_train_tensors,model,feature_extractor,100)"
      ],
      "metadata": {
        "id": "bU8afQ8WmfgO",
        "outputId": "442607b6-802f-4697-c7b7-355bf9624f01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50000it [15:58, 52.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A dataset for the distillation training.\n",
        "    This dataset wraps the original dataset toghether with the teacher ground truth.\n",
        "    '''\n",
        "    def __init__(self, \n",
        "                 original_dataset : torch.utils.data.Dataset,\n",
        "                 teacher_label_path : Path = None,\n",
        "                 teacher_model : torch.nn.Module = None,\n",
        "                 teacher_extractor : Any = None,\n",
        "                 save : bool = False\n",
        "                 ):\n",
        "        assert(teacher_label_path is not None or teacher_model is not None)\n",
        "        super().__init__()\n",
        "        self.dataset = original_dataset\n",
        "        try:\n",
        "            with open(teacher_label_path, \"rb\") as dl:\n",
        "                self.teacher_label = torch.load(dl)\n",
        "        except:\n",
        "            self.teacher_label = compute_scores(self.dataset, \n",
        "                                                teacher_model,\n",
        "                                                teacher_extractor,\n",
        "                                                len(self.dataset.classes)\n",
        "                                                )\n",
        "            if save:\n",
        "                with open(teacher_label_path, \"wb\") as tl:\n",
        "                    torch.save(self.teacher_label,tl)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.dataset[index][0], self.dataset[index][1] , self.teacher_label[index]"
      ],
      "metadata": {
        "id": "dpwMnJ1j6ODN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DistillDataset(cifar100_train_tensors, \n",
        "                         \"teacher_labels/cifar100.pt\"\n",
        "                         )"
      ],
      "metadata": {
        "id": "E3gGWUDJxw9H"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "5hYbJen4yEGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --user Ale99 --password Vitmlpmixer99 https://huggingface.co/datasets/imagenet-1k/resolve/main/data/val_images.tar.gz"
      ],
      "metadata": {
        "id": "zZSrAlsIyF3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imagenet_val\n",
        "!tar -xvf val_images.tar.gz -C imagenet_val"
      ],
      "metadata": {
        "id": "9OBoo-ad6082"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}